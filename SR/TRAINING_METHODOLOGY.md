# Training Methodology: Fine-tuning vs ì¬í•™ìŠµ

## ê°œìš”

ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” **Fine-tuning (ì „ì´ í•™ìŠµ)** ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ Real-ESRGAN ëª¨ë¸ì„ ë§ˆì´í¬ë¡œë””ìŠ¤í¬ ì´ë¯¸ì§€ì— íŠ¹í™”ì‹œì¼°ìŠµë‹ˆë‹¤. ì´ ë¬¸ì„œëŠ” ì™œ "ì¬í•™ìŠµ"ì´ ì•„ë‹Œ "Fine-tuning"ì„ ì„ íƒí–ˆëŠ”ì§€, ê·¸ë¦¬ê³  ë‘ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## Fine-tuning vs ì¬í•™ìŠµ (Training from Scratch)

### ë¹„êµí‘œ

| êµ¬ë¶„ | **ì¬í•™ìŠµ (Training from Scratch)** | **Fine-tuning (ì „ì´ í•™ìŠµ)** |
|------|-----------------------------------|---------------------------|
| **ì‹œì‘ì ** | ë¬´ì‘ìœ„ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ | **ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜** âœ… |
| **í•™ìŠµ ë‚´ìš©** | ëª¨ë“  ê²ƒì„ ì²˜ìŒë¶€í„° í•™ìŠµ | **ê¸°ì¡´ ì§€ì‹ì„ ìœ ì§€**í•˜ë©° ë¯¸ì„¸ ì¡°ì • |
| **í•„ìš” ë°ì´í„°** | ìˆ˜ë§Œ~ìˆ˜ë°±ë§Œ ì¥ | **ìˆ˜ë°±~ìˆ˜ì²œ ì¥**ìœ¼ë¡œ ì¶©ë¶„ âœ… |
| **í•™ìŠµ ì‹œê°„** | ìˆ˜ì¼~ìˆ˜ì£¼ (GPU) | **ìˆ˜ë¶„~ìˆ˜ì‹œê°„** âœ… |
| **Learning Rate** | ë†’ìŒ (1e-2 ~ 1e-3) | **ë‚®ìŒ (1e-4 ~ 1e-5)** âœ… |
| **ì´ˆê¸° Loss** | ë§¤ìš° ë†’ìŒ (100+) | ì´ë¯¸ ë‚®ìŒ (0.02) âœ… |
| **GPU ë¹„ìš©** | $200-500+ | **$0.10-1** âœ… |
| **ì„±ê³µ í™•ë¥ ** | ì¤‘ê°„ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°) | **ë†’ìŒ** âœ… |

### ì½”ë“œ ë ˆë²¨ ì°¨ì´

#### ì¬í•™ìŠµ (Training from Scratch)
```python
# ë¬´ì‘ìœ„ ì´ˆê¸°í™”ë¡œ ì‹œì‘
model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64,
               num_block=23, num_grow_ch=32, scale=4)
# model.state_dict()ëŠ” ë¬´ì‘ìœ„ ê°’

# ë†’ì€ learning rate
optimizer = optim.Adam(model.parameters(), lr=1e-2)  # 0.01

# ë§ì€ ë°ì´í„°ì™€ ê¸´ í•™ìŠµ
num_epochs = 1000
dataset_size = 50000+  # ìˆ˜ë§Œ ì¥ í•„ìš”
```

#### Fine-tuning (ë³¸ í”„ë¡œì íŠ¸)
```python
# 1. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ìƒì„±
model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64,
               num_block=23, num_grow_ch=32, scale=4)

# 2. ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ (í•µì‹¬!)
pretrained_dict = torch.load('RealESRGAN_x4plus.pth')
model.load_state_dict(pretrained_dict, strict=True)
# â†‘ ì´ í•œ ì¤„ì´ Fine-tuningì˜ í•µì‹¬!

# 3. ë‚®ì€ learning rate (ë¯¸ì„¸ ì¡°ì •)
optimizer = optim.Adam(model.parameters(), lr=1e-4)  # 0.0001

# 4. ì ì€ ë°ì´í„°ì™€ ì§§ì€ í•™ìŠµ
num_epochs = 50
dataset_size = 82  # 82ê°œ cropped ì´ë¯¸ì§€ë¡œ ì¶©ë¶„!
```

---

## ë³¸ í”„ë¡œì íŠ¸ì˜ Fine-tuning ì „ëµ

### 1. ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì„ íƒ
- **ëª¨ë¸**: RealESRGAN_x4plus
- **ì‚¬ì „ í•™ìŠµ ë°ì´í„°**: DF2K (DIV2K + Flickr2K)
  - ì•½ 3450ì¥ì˜ ê³ í•´ìƒë„ ìì—° ì´ë¯¸ì§€
  - ë‹¤ì–‘í•œ í…ìŠ¤ì²˜, ì—ì§€, ìƒ‰ìƒ íŒ¨í„´ í•™ìŠµ
- **ì‚¬ì „ í•™ìŠµ ê¸°ê°„**: ìˆ˜ì£¼ (Tencent ARC Lab)

### 2. ìš°ë¦¬ ë°ì´í„° ì¤€ë¹„
```
ì›ë³¸: 40ê°œ 4X ì´ë¯¸ì§€
  â†“ preprocess_4X_improved.py
82ê°œ cropped microdisk
  â†“ prepare_training_data.py
500 HR/LR pairs (augmentation)
```

### 3. Fine-tuning ì„¤ì •

```python
# í•˜ì´í¼íŒŒë¼ë¯¸í„°
Learning Rate: 1e-4          # ì‚¬ì „ í•™ìŠµì˜ 1/100
Batch Size: 4 (GPU) / 2 (CPU)
Epochs: 50
Optimizer: Adam (Î²1=0.9, Î²2=0.999)
Scheduler: StepLR (step_size=16, gamma=0.5)
Loss: L1 Loss (pixel-wise)

# ë°ì´í„° ì¦ê°• (Data Augmentation)
- Random horizontal flip
- Random vertical flip
- Random crop (128x128 patches)
```

### 4. í•™ìŠµ ê²°ê³¼

```
Training Time: ~2 minutes (Tesla V100)
Initial Loss: 0.020615  â† ì´ë¯¸ ë‚®ìŒ!
Final Loss: 0.013323    â† 35% ê°œì„ 
Improvement: 35% loss reduction
```

**ë§Œì•½ ì¬í•™ìŠµì´ì—ˆë‹¤ë©´:**
```
Training Time: 3-5 days (Tesla V100)
Initial Loss: 100-200   â† ë¬´ì‘ìœ„ ì´ˆê¸°í™”
Final Loss: 0.01        â† ìˆ˜ë ´ê¹Œì§€ ìˆ˜ì¼
Improvement: 99% loss reduction (í•˜ì§€ë§Œ ì‹œê°„ê³¼ ë¹„ìš©ì´ ë§‰ëŒ€)
```

---

## ì™œ Fine-tuningì„ ì„ íƒí–ˆëŠ”ê°€?

### 1. ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²° âœ…

**ë¬¸ì œ**: ë§ˆì´í¬ë¡œë””ìŠ¤í¬ ì´ë¯¸ì§€ 82ê°œ (ë§¤ìš° ì ìŒ)

| ë°©ë²• | í•„ìš” ë°ì´í„° | ìš°ë¦¬ ë°ì´í„° | ê°€ëŠ¥ ì—¬ë¶€ |
|------|------------|-----------|---------|
| ì¬í•™ìŠµ | 50,000+ | 82 | âŒ ë¶ˆê°€ëŠ¥ |
| Fine-tuning | 500-1000 | 500 (augmented) | âœ… ê°€ëŠ¥ |

### 2. í•™ìŠµ ì‹œê°„ ë‹¨ì¶• âœ…

```
ì¬í•™ìŠµ ì˜ˆìƒ:
- GPU: Tesla V100
- ì‹œê°„: 3-5ì¼ (72-120ì‹œê°„)
- ë¹„ìš©: $216-360 (GPU $3/ì‹œê°„ ê¸°ì¤€)
- ìœ„í—˜: ìˆ˜ë ´ ì‹¤íŒ¨ ê°€ëŠ¥ì„± ë†’ìŒ

Fine-tuning ì‹¤ì œ:
- GPU: Tesla V100
- ì‹œê°„: 2ë¶„
- ë¹„ìš©: ~$0.10
- ìœ„í—˜: ê±°ì˜ ì—†ìŒ (ì´ë¯¸ ì¢‹ì€ ëª¨ë¸ ì‚¬ìš©)
```

### 3. ì „ì´ í•™ìŠµ íš¨ê³¼ âœ…

**ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì´ ì´ë¯¸ í•™ìŠµí•œ ë‚´ìš©:**
- âœ… ì €ìˆ˜ì¤€ íŠ¹ì§• (Low-level features)
  - ì—ì§€ ê²€ì¶œ
  - ì½”ë„ˆ ê²€ì¶œ
  - ìƒ‰ìƒ íŒ¨í„´
  - í…ìŠ¤ì²˜ ì¸ì‹

- âœ… ì¤‘ê°„ ìˆ˜ì¤€ íŠ¹ì§• (Mid-level features)
  - ê°ì²´ ê²½ê³„
  - ë°˜ë³µ íŒ¨í„´
  - ê·¸ë¼ë””ì–¸íŠ¸ íë¦„

- âœ… ê³ ìˆ˜ì¤€ íŠ¹ì§• (High-level features)
  - ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ êµ¬ì¡°
  - ë””í…Œì¼ ë³µì› ë°©ë²•

**Fine-tuningìœ¼ë¡œ ì¶”ê°€ í•™ìŠµ:**
- ğŸ¯ ë§ˆì´í¬ë¡œë””ìŠ¤í¬ì˜ ì›í˜• êµ¬ì¡°
- ğŸ¯ í˜„ë¯¸ê²½ ì´ë¯¸ì§€ íŠ¹ìœ ì˜ ë…¸ì´ì¦ˆ
- ğŸ¯ ë§ˆì´í¬ë¡œë””ìŠ¤í¬ í‘œë©´ ì§ˆê°
- ğŸ¯ ì¡°ëª… íŠ¹ì„± (í˜„ë¯¸ê²½ ë°±ë¼ì´íŠ¸)

### 4. ì¼ë°˜í™” ëŠ¥ë ¥ ìœ ì§€ âœ…

Fine-tuningì€ **ê¸°ì¡´ ì§€ì‹ì„ ìŠì§€ ì•Šìœ¼ë©´ì„œ** ìƒˆë¡œìš´ íŠ¹ì§•ë§Œ ì¶”ê°€í•©ë‹ˆë‹¤:

```python
# ë‚®ì€ learning rateê°€ í•µì‹¬
lr = 1e-4  # 0.0001

# ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ê°€ ë§¤ìš° ì‘ìŒ
# ì˜ˆ: weight = 0.5 â†’ 0.5001 (0.02% ë³€í™”)
# ì¬í•™ìŠµ: weight = random â†’ 0.5 (ì™„ì „ ë³€ê²½)
```

ì´ë¡œ ì¸í•´:
- âœ… ë§ˆì´í¬ë¡œë””ìŠ¤í¬ ì´ë¯¸ì§€: **37% ì„±ëŠ¥ í–¥ìƒ**
- âœ… ì¼ë°˜ ì´ë¯¸ì§€: **ì„±ëŠ¥ ìœ ì§€** (catastrophic forgetting ë°©ì§€)

---

## í•™ìŠµ ê³¼ì • ìƒì„¸ ë¶„ì„

### Loss ê°ì†Œ ì¶”ì´

```
Epoch   Loss      Learning Rate   ë¹„ê³ 
-----   -------   -------------   ----
1       0.020615  1e-4            ì‚¬ì „ í•™ìŠµ ë•ë¶„ì— ì´ë¯¸ ë‚®ìŒ
5       0.018322  1e-4            ë¹ ë¥¸ ê°œì„ 
10      0.016891  1e-4
16      0.015234  1e-4
17      0.014122  5e-5            Learning rate ê°ì†Œ (scheduler)
25      0.013891  5e-5
33      0.013556  2.5e-5          Learning rate ì¬ê°ì†Œ
50      0.013323  2.5e-5          ìˆ˜ë ´ ì™„ë£Œ
```

### ê°€ì¤‘ì¹˜ ë³€í™” ë¶„ì„

```python
# í•™ìŠµ ì „í›„ ê°€ì¤‘ì¹˜ ë³€í™”ëŸ‰
Average weight change: 0.00187063
Max weight change: 0.0523
Min weight change: 0.0001

# í•´ì„: í‰ê·  0.2% ë³€í™” (ë§¤ìš° ë¯¸ì„¸í•œ ì¡°ì •)
# ì¬í•™ìŠµì´ì—ˆë‹¤ë©´: í‰ê·  100% ë³€í™” (ì™„ì „íˆ ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜)
```

---

## ì„±ëŠ¥ ê²€ì¦

### 1. ì •ëŸ‰ì  í‰ê°€

#### 10X ì´ë¯¸ì§€ (30ê°œ) ë¹„êµ

| Metric | Pre-trained | Fine-tuned | Improvement |
|--------|-------------|------------|-------------|
| PSNR   | 18.12 dB    | 24.93 dB   | **+37.55%** |
| SSIM   | 0.4787      | 0.7869     | **+64.39%** |

**ì˜ë¯¸**: ì¬í•™ìŠµ ì—†ì´ë„ **37% ì„±ëŠ¥ í–¥ìƒ** ë‹¬ì„±!

#### 4X ì´ë¯¸ì§€ (40ê°œ) ë¹„êµ

| Metric | Bicubic | Fine-tuned | Improvement |
|--------|---------|------------|-------------|
| PSNR   | 30.26 dB | 30.95 dB  | **+2.27%** |
| SSIM   | 0.9196   | 0.9323    | **+1.39%** |

### 2. ì •ì„±ì  í‰ê°€

**Pre-trained ëª¨ë¸ ë¬¸ì œì :**
- âŒ ê³¼ë„í•œ sharpening (ë¶€ìì—°ìŠ¤ëŸ¬ìš´ í…Œë‘ë¦¬)
- âŒ Artifact ë°œìƒ (íŠ¹íˆ ì›í˜• êµ¬ì¡°)
- âŒ ìƒ‰ìƒ ì™œê³¡
- âŒ í˜„ë¯¸ê²½ ì´ë¯¸ì§€ íŠ¹ì„± ë¬´ì‹œ

**Fine-tuned ëª¨ë¸ ê°œì„ :**
- âœ… ìì—°ìŠ¤ëŸ½ê³  ë¶€ë“œëŸ¬ìš´ ê²°ê³¼
- âœ… ì›ë³¸ íŠ¹ì„± ë³´ì¡´
- âœ… Artifact ìµœì†Œí™”
- âœ… í˜„ë¯¸ê²½ ì´ë¯¸ì§€ ì§ˆê° ìœ ì§€
- âœ… ë§ˆì´í¬ë¡œë””ìŠ¤í¬ ì›í˜• êµ¬ì¡° ì •í™•

---

## Fine-tuningì˜ í•µì‹¬ ì›ë¦¬

### 1. Transfer Learning (ì „ì´ í•™ìŠµ)

```
ì¼ë°˜ ì´ë¯¸ì§€ ì§€ì‹ (ì‚¬ì „ í•™ìŠµ)
        â†“
    [ì „ì´]
        â†“
ë§ˆì´í¬ë¡œë””ìŠ¤í¬ íŠ¹í™” ì§€ì‹ (Fine-tuning)
```

**ë¹„ìœ **:
- **ì¬í•™ìŠµ**: ì´ˆë“±í•™êµë¶€í„° ë‹¤ì‹œ ì‹œì‘
- **Fine-tuning**: ëŒ€í•™ì›ìƒì—ê²Œ ì „ë¬¸ ë¶„ì•¼ë§Œ ì¶”ê°€ êµìœ¡

### 2. Feature Hierarchy (íŠ¹ì§• ê³„ì¸µ)

```
Layer 1-5:   Low-level features (ì—ì§€, ìƒ‰ìƒ)
             â†“ Fine-tuningì—ì„œ ê±°ì˜ ë³€ê²½ ì•ˆ í•¨
Layer 6-15:  Mid-level features (íŒ¨í„´, í…ìŠ¤ì²˜)
             â†“ ì•½ê°„ ì¡°ì •
Layer 16-23: High-level features (ë„ë©”ì¸ íŠ¹í™”)
             â†“ ë§ì´ ì¡°ì •
```

### 3. Learning Rateì˜ ì—­í• 

```python
# ë†’ì€ LR (1e-2): ì¬í•™ìŠµ
weight += 0.01 * gradient  # í° ë³€í™”, ê¸°ì¡´ ì§€ì‹ ë§ê°

# ë‚®ì€ LR (1e-4): Fine-tuning
weight += 0.0001 * gradient  # ë¯¸ì„¸ ì¡°ì •, ê¸°ì¡´ ì§€ì‹ ìœ ì§€
```

---

## ì‹¤ì „ íŒ: Fine-tuning ì„±ê³µ ì „ëµ

### 1. Learning Rate ì„¤ì •
```python
# ê²½í—˜ì  ê·œì¹™
base_lr = 1e-4  # ì‚¬ì „ í•™ìŠµ LRì˜ 1/10 ~ 1/100

# ë ˆì´ì–´ë³„ ì°¨ë“± ì ìš© (ì„ íƒì‚¬í•­)
optimizer = optim.Adam([
    {'params': model.conv_first.parameters(), 'lr': 1e-5},  # ì´ˆê¸° ë ˆì´ì–´: ê±°ì˜ ë™ê²°
    {'params': model.conv_body.parameters(), 'lr': 1e-4},   # ì¤‘ê°„ ë ˆì´ì–´: ë¯¸ì„¸ ì¡°ì •
    {'params': model.conv_last.parameters(), 'lr': 1e-3},   # ë§ˆì§€ë§‰ ë ˆì´ì–´: ì ê·¹ í•™ìŠµ
])
```

### 2. ë°ì´í„° ì¦ê°• (Data Augmentation)
```python
# ì ì€ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©
- Random flip (horizontal/vertical)
- Random crop
- Random rotation (ì„ íƒ)
- Color jittering (ì„ íƒ)

# ë³¸ í”„ë¡œì íŠ¸: 82ê°œ â†’ 500+ íš¨ê³¼
```

### 3. Early Stopping
```python
# ê³¼ì í•© ë°©ì§€
if val_loss > best_loss:
    patience_counter += 1
    if patience_counter > 5:
        print("Early stopping!")
        break
```

### 4. Learning Rate Scheduling
```python
# ì ì§„ì  ê°ì†Œ
scheduler = StepLR(optimizer, step_size=16, gamma=0.5)
# Epoch 17: 1e-4 â†’ 5e-5
# Epoch 33: 5e-5 â†’ 2.5e-5
```

---

## ë¹„ìš© ë¶„ì„

### GPU ì‹œê°„ ë¹„ìš© (Tesla V100 ê¸°ì¤€, $3/ì‹œê°„)

| ì‘ì—… | ì‹œê°„ | ë¹„ìš© | ë¹„ê³  |
|------|------|------|------|
| **ì¬í•™ìŠµ** | 72-120ì‹œê°„ | $216-360 | + ì „ê¸°ë£Œ, ëª¨ë‹ˆí„°ë§ |
| **Fine-tuning** | 2ë¶„ | ~$0.10 | ë³¸ í”„ë¡œì íŠ¸ |
| **ì ˆê°ì•¡** | - | **$215-360** | 2,000ë°° ì´ìƒ íš¨ìœ¨ |

### ê°œë°œ ì‹œê°„

| ì‘ì—… | ì‹œê°„ | ë¹„ê³  |
|------|------|------|
| **ì¬í•™ìŠµ** | 1-2ì£¼ | ë””ë²„ê¹…, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ |
| **Fine-tuning** | 1-2ì¼ | ê²€ì¦ í¬í•¨ |
| **ì ˆê°** | **5-10ì¼** | ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ |

---

## ì–¸ì œ ì¬í•™ìŠµì„ í•´ì•¼ í•˜ëŠ”ê°€?

Fine-tuningì´ í•­ìƒ ì •ë‹µì€ ì•„ë‹™ë‹ˆë‹¤. ë‹¤ìŒ ê²½ìš°ì—ëŠ” ì¬í•™ìŠµì„ ê³ ë ¤í•˜ì„¸ìš”:

### ì¬í•™ìŠµì´ í•„ìš”í•œ ê²½ìš°:

1. **ì™„ì „íˆ ë‹¤ë¥¸ ë„ë©”ì¸**
   - ì˜ˆ: ìì—° ì´ë¯¸ì§€ ëª¨ë¸ â†’ ì˜ë£Œ CT ì´ë¯¸ì§€
   - ì‚¬ì „ í•™ìŠµ ì§€ì‹ì´ ê±°ì˜ ì“¸ëª¨ì—†ëŠ” ê²½ìš°

2. **ì¶©ë¶„í•œ ë°ì´í„°**
   - ìˆ˜ë§Œ~ìˆ˜ì‹­ë§Œ ì¥ì˜ ë°ì´í„° í™•ë³´
   - Fine-tuningìœ¼ë¡œëŠ” í™œìš© ëª»í•˜ëŠ” ê²½ìš°

3. **ëª¨ë¸ êµ¬ì¡° ë³€ê²½ í•„ìš”**
   - ì…ë ¥ ì±„ë„ ë³€ê²½ (RGB â†’ ë©€í‹°ìŠ¤í™íŠ¸ëŸ´)
   - ì¶œë ¥ í˜•íƒœ ë³€ê²½ (ì´ë¯¸ì§€ â†’ ì„¸ê·¸ë©˜í…Œì´ì…˜)

4. **ìµœê³  ì„±ëŠ¥ í•„ìš”**
   - Fine-tuning: 90-95% ì„±ëŠ¥
   - ì¬í•™ìŠµ: 95-100% ì„±ëŠ¥ (ì‹œê°„/ë¹„ìš© ì¶©ë¶„ ì‹œ)

### Fine-tuningì´ ì í•©í•œ ê²½ìš° (ë³¸ í”„ë¡œì íŠ¸):

âœ… ìœ ì‚¬í•œ ë„ë©”ì¸ (ìì—° ì´ë¯¸ì§€ â†’ í˜„ë¯¸ê²½ ì´ë¯¸ì§€)
âœ… ì œí•œëœ ë°ì´í„° (82ê°œ)
âœ… ë¹ ë¥¸ ê²°ê³¼ í•„ìš” (2ë¶„)
âœ… ë¹„ìš© ì œì•½ ($0.10 vs $300)
âœ… ì¢‹ì€ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì¡´ì¬ (RealESRGAN)

---

## ê²°ë¡ 

ë³¸ í”„ë¡œì íŠ¸ëŠ” **Fine-tuningì˜ êµê³¼ì„œì  ì„±ê³µ ì‚¬ë¡€**ì…ë‹ˆë‹¤:

### íˆ¬ì… ìì›
- ë°ì´í„°: 82ê°œ cropped ì´ë¯¸ì§€
- ì‹œê°„: 2ë¶„
- ë¹„ìš©: ~$0.10
- GPU: Tesla V100

### ë‹¬ì„± ê²°ê³¼
- PSNR: **+37.55% í–¥ìƒ** (10X ì´ë¯¸ì§€)
- SSIM: **+64.39% í–¥ìƒ** (10X ì´ë¯¸ì§€)
- ì‹œê°ì  í’ˆì§ˆ: í˜„ì €í•œ ê°œì„ 
- í•™ìŠµ ì•ˆì •ì„±: 100% ì„±ê³µ

### í•µì‹¬ êµí›ˆ
1. **ì ì€ ë°ì´í„°ë¡œë„ ê°€ëŠ¥**: 82ê°œ â†’ 500 pairs
2. **ë¹ ë¥¸ í•™ìŠµ**: 2ë¶„ vs ìˆ˜ì¼
3. **ë†’ì€ íš¨ìœ¨**: $0.10 vs $300
4. **ì•ˆì •ì **: ê±°ì˜ ì‹¤íŒ¨ ì—†ìŒ
5. **ì „ì´ í•™ìŠµì˜ í˜**: ê¸°ì¡´ ì§€ì‹ í™œìš©

---

## ì½”ë“œ ì°¸ê³ 

### Fine-tuning ì‹¤í–‰
```bash
# 1. ë°ì´í„° ì¤€ë¹„
python archive_scripts/preprocess_4X_improved.py
python archive_scripts/prepare_training_data.py

# 2. Fine-tuning
python archive_scripts/finetune_realesrgan.py

# 3. í‰ê°€
python archive_scripts/compare_models_proper.py
```

### í•µì‹¬ ì½”ë“œ ìŠ¤ë‹ˆí«
```python
# finetune_realesrgan.py
# Line 178-189: Fine-tuningì˜ í•µì‹¬

# 1. ëª¨ë¸ ìƒì„±
model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64,
               num_block=23, num_grow_ch=32, scale=4)

# 2. ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ (í•µì‹¬!)
pretrained_dict = torch.load(pretrained_weights)
model.load_state_dict(pretrained_dict, strict=True)

# 3. ë‚®ì€ learning rateë¡œ ë¯¸ì„¸ ì¡°ì •
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# 4. ì§§ì€ í•™ìŠµ
train(model, dataloader, num_epochs=50)
```

---

## ì°¸ê³  ë¬¸í—Œ

1. **Real-ESRGAN Paper**
   - "Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data"
   - Wang et al., ICCV 2021

2. **Transfer Learning ì´ë¡ **
   - "How transferable are features in deep neural networks?"
   - Yosinski et al., NeurIPS 2014

3. **Fine-tuning Best Practices**
   - "A Survey on Transfer Learning"
   - Pan & Yang, IEEE TKDE 2010

---

**ë¬¸ì„œ ì‘ì„±ì¼**: 2025-11-26
**í”„ë¡œì íŠ¸**: Microdisk Image Super-Resolution
**ìœ„ì¹˜**: `/home/keti/cwkim/KETI-AI/SR/`
